{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZG6rcrBN2Zm2QmRa2G60y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amyliaw/CS6220-Data-Mining-Project/blob/master/Basic_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FYfJbOHOuB28"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-C6-_v8su5ca",
        "outputId": "59a312fc-e5c7-46bb-f0d1-b2ae2000c6c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample text data\n",
        "texts = [\n",
        "    \"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\",\n",
        "    \"Preprocessing text data is an essential step in natural language processing (NLP) tasks such as sentiment analysis, text classification, and machine translation.\",\n",
        "    \"NLTK provides easy-to-use tools for text preprocessing, including tokenization, stopword removal, and lemmatization.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "YQVQHDefvRe7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Tokenization\n",
        "# at this step, we separate individual words and punctuation from the sentences\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]\n",
        "print(tokenized_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hysfhuRQvZ7D",
        "outputId": "4b6715bd-0c60-4434-ad7c-b39be7e5d8c7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'language', ',', 'in', 'particular', 'how', 'to', 'program', 'computers', 'to', 'process', 'and', 'analyze', 'large', 'amounts', 'of', 'natural', 'language', 'data', '.'], ['preprocessing', 'text', 'data', 'is', 'an', 'essential', 'step', 'in', 'natural', 'language', 'processing', '(', 'nlp', ')', 'tasks', 'such', 'as', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'and', 'machine', 'translation', '.'], ['nltk', 'provides', 'easy-to-use', 'tools', 'for', 'text', 'preprocessing', ',', 'including', 'tokenization', ',', 'stopword', 'removal', ',', 'and', 'lemmatization', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [[word for word in text if word not in stop_words] for text in tokenized_texts]\n",
        "print(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkfhKSqEvZr9",
        "outputId": "8a4daf0c-7ad1-4483-ef71-06d2cd9ec86e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['natural', 'language', 'processing', '(', 'nlp', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interactions', 'computers', 'human', 'language', ',', 'particular', 'program', 'computers', 'process', 'analyze', 'large', 'amounts', 'natural', 'language', 'data', '.'], ['preprocessing', 'text', 'data', 'essential', 'step', 'natural', 'language', 'processing', '(', 'nlp', ')', 'tasks', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'machine', 'translation', '.'], ['nltk', 'provides', 'easy-to-use', 'tools', 'text', 'preprocessing', ',', 'including', 'tokenization', ',', 'stopword', 'removal', ',', 'lemmatization', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_texts = [[lemmatizer.lemmatize(word) for word in text] for text in filtered_words]\n",
        "print(lemmatized_texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmQV55ZlvZeJ",
        "outputId": "fe470b14-175b-4284-c4e0-ca143bd9baa9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['natural', 'language', 'processing', '(', 'nlp', ')', 'subfield', 'linguistics', ',', 'computer', 'science', ',', 'artificial', 'intelligence', 'concerned', 'interaction', 'computer', 'human', 'language', ',', 'particular', 'program', 'computer', 'process', 'analyze', 'large', 'amount', 'natural', 'language', 'data', '.'], ['preprocessing', 'text', 'data', 'essential', 'step', 'natural', 'language', 'processing', '(', 'nlp', ')', 'task', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'machine', 'translation', '.'], ['nltk', 'provides', 'easy-to-use', 'tool', 'text', 'preprocessing', ',', 'including', 'tokenization', ',', 'stopword', 'removal', ',', 'lemmatization', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the preprocessed texts\n",
        "for i, text in enumerate(lemmatized_texts):\n",
        "    print(f\"Preprocessed Text {i+1}: {' '.join(text)}\\n\")\n",
        "\n",
        "# Convert preprocessed texts to a bag-of-words representation\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform([' '.join(text) for text in lemmatized_texts])\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Display the bag-of-words matrix\n",
        "print(\"Bag-of-Words Matrix:\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWZFUX4AySFt",
        "outputId": "68f20964-7920-44ec-a90d-eef2d664b18a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Text 1: natural language processing ( nlp ) subfield linguistics , computer science , artificial intelligence concerned interaction computer human language , particular program computer process analyze large amount natural language data .\n",
            "\n",
            "Preprocessed Text 2: preprocessing text data essential step natural language processing ( nlp ) task sentiment analysis , text classification , machine translation .\n",
            "\n",
            "Preprocessed Text 3: nltk provides easy-to-use tool text preprocessing , including tokenization , stopword removal , lemmatization .\n",
            "\n",
            "Bag-of-Words Matrix:\n",
            "   amount  analysis  analyze  artificial  classification  computer  concerned  \\\n",
            "0       1         0        1           1               0         3          1   \n",
            "1       0         1        0           0               1         0          0   \n",
            "2       0         0        0           0               0         0          0   \n",
            "\n",
            "   data  easy  essential  ...  step  stopword  subfield  task  text  to  \\\n",
            "0     1     0          0  ...     0         0         1     0     0   0   \n",
            "1     1     0          1  ...     1         0         0     1     2   0   \n",
            "2     0     1          0  ...     0         1         0     0     1   1   \n",
            "\n",
            "   tokenization  tool  translation  use  \n",
            "0             0     0            0    0  \n",
            "1             0     0            1    0  \n",
            "2             1     1            0    1  \n",
            "\n",
            "[3 rows x 41 columns]\n"
          ]
        }
      ]
    }
  ]
}